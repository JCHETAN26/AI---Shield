{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e2abe28",
   "metadata": {},
   "source": [
    "# AI Shield - Adversarial ML Security Analysis Demo\n",
    "\n",
    "This notebook demonstrates how to use AI Shield for adversarial machine learning security analysis.\n",
    "\n",
    "## Features Demonstrated:\n",
    "- Loading models and data from S3\n",
    "- Running FGSM and PGD adversarial attacks\n",
    "- Generating SHAP and LIME explanations\n",
    "- Analyzing vulnerabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89b4a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if running in SageMaker\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_package(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Uncomment the following lines if packages are not installed\n",
    "# install_package(\"adversarial-robustness-toolbox\")\n",
    "# install_package(\"shap\")\n",
    "# install_package(\"lime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda5adc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# AI Shield imports\n",
    "from src.utils.data_processor import DataProcessor\n",
    "from src.utils.model_loader import ModelLoader\n",
    "from src.adversarial.attack_engine import AdversarialAttackEngine\n",
    "from src.xai.explanation_engine import XAIExplanationEngine\n",
    "\n",
    "print(\"AI Shield modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582c5743",
   "metadata": {},
   "source": [
    "## Step 1: Create Sample Data and Model\n",
    "\n",
    "For this demo, we'll create synthetic data and train a simple model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b87207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize components\n",
    "data_processor = DataProcessor()\n",
    "model_loader = ModelLoader()\n",
    "attack_engine = AdversarialAttackEngine()\n",
    "xai_engine = XAIExplanationEngine()\n",
    "\n",
    "# Create synthetic dataset\n",
    "data = data_processor.create_synthetic_data(\n",
    "    n_samples=1000,\n",
    "    n_features=10,\n",
    "    n_classes=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Dataset created:\")\n",
    "print(f\"- Training samples: {len(data['X_train'])}\")\n",
    "print(f\"- Test samples: {len(data['X_test'])}\")\n",
    "print(f\"- Features: {len(data['feature_names'])}\")\n",
    "print(f\"- Classes: {data['num_classes']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c0692f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a simple model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(data['X_train'], data['y_train'])\n",
    "\n",
    "# Evaluate baseline performance\n",
    "train_accuracy = model.score(data['X_train'], data['y_train'])\n",
    "test_accuracy = model.score(data['X_test'], data['y_test'])\n",
    "\n",
    "print(f\"Model Performance:\")\n",
    "print(f\"- Training Accuracy: {train_accuracy:.3f}\")\n",
    "print(f\"- Test Accuracy: {test_accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee4eae8",
   "metadata": {},
   "source": [
    "## Step 2: Run Adversarial Attacks\n",
    "\n",
    "Now we'll test the model's robustness using FGSM and PGD attacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c198cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run FGSM attack\n",
    "print(\"Running FGSM attack...\")\n",
    "try:\n",
    "    fgsm_results = attack_engine.run_fgsm_attack(\n",
    "        model=model,\n",
    "        X_test=data['X_test'],\n",
    "        y_test=data['y_test'],\n",
    "        epsilon=0.1,\n",
    "        framework='sklearn'\n",
    "    )\n",
    "    \n",
    "    print(f\"FGSM Attack Results:\")\n",
    "    print(f\"- Original Accuracy: {fgsm_results['original_accuracy']:.3f}\")\n",
    "    print(f\"- Adversarial Accuracy: {fgsm_results['adversarial_accuracy']:.3f}\")\n",
    "    print(f\"- Attack Success Rate: {fgsm_results['success_rate']:.3f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"FGSM attack failed: {e}\")\n",
    "    # Create mock results for demonstration\n",
    "    fgsm_results = {\n",
    "        'attack_type': 'FGSM',\n",
    "        'original_accuracy': test_accuracy,\n",
    "        'adversarial_accuracy': test_accuracy * 0.8,\n",
    "        'success_rate': 0.2,\n",
    "        'adversarial_examples': data['X_test'][:10].tolist()\n",
    "    }\n",
    "    print(\"Using mock FGSM results for demonstration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9cbaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run PGD attack\n",
    "print(\"Running PGD attack...\")\n",
    "try:\n",
    "    pgd_results = attack_engine.run_pgd_attack(\n",
    "        model=model,\n",
    "        X_test=data['X_test'],\n",
    "        y_test=data['y_test'],\n",
    "        epsilon=0.1,\n",
    "        alpha=0.01,\n",
    "        max_iter=20,\n",
    "        framework='sklearn'\n",
    "    )\n",
    "    \n",
    "    print(f\"PGD Attack Results:\")\n",
    "    print(f\"- Original Accuracy: {pgd_results['original_accuracy']:.3f}\")\n",
    "    print(f\"- Adversarial Accuracy: {pgd_results['adversarial_accuracy']:.3f}\")\n",
    "    print(f\"- Attack Success Rate: {pgd_results['success_rate']:.3f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"PGD attack failed: {e}\")\n",
    "    # Create mock results for demonstration\n",
    "    pgd_results = {\n",
    "        'attack_type': 'PGD',\n",
    "        'original_accuracy': test_accuracy,\n",
    "        'adversarial_accuracy': test_accuracy * 0.7,\n",
    "        'success_rate': 0.3,\n",
    "        'adversarial_examples': data['X_test'][:10].tolist()\n",
    "    }\n",
    "    print(\"Using mock PGD results for demonstration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0eb288c",
   "metadata": {},
   "source": [
    "## Step 3: Generate XAI Explanations\n",
    "\n",
    "We'll use SHAP and LIME to understand what makes the model vulnerable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53e1f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate SHAP explanations\n",
    "print(\"Generating SHAP explanations...\")\n",
    "attack_results = {'fgsm': fgsm_results, 'pgd': pgd_results}\n",
    "\n",
    "shap_results = xai_engine.generate_shap_explanations(\n",
    "    model=model,\n",
    "    X_test=data['X_test'],\n",
    "    attack_results=attack_results,\n",
    "    feature_names=data['feature_names'],\n",
    "    max_samples=50\n",
    ")\n",
    "\n",
    "print(f\"SHAP Analysis Complete:\")\n",
    "print(f\"- Explainer Type: {shap_results.get('explainer_type', 'Unknown')}\")\n",
    "print(f\"- Top 5 Important Features: {shap_results.get('important_features', [])[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534bb1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate LIME explanations\n",
    "print(\"Generating LIME explanations...\")\n",
    "\n",
    "lime_results = xai_engine.generate_lime_explanations(\n",
    "    model=model,\n",
    "    X_test=data['X_test'],\n",
    "    feature_names=data['feature_names'],\n",
    "    attack_results=attack_results,\n",
    "    max_samples=20\n",
    ")\n",
    "\n",
    "print(f\"LIME Analysis Complete:\")\n",
    "print(f\"- Explanations Generated: {lime_results.get('num_explanations', 0)}\")\n",
    "print(f\"- Top Important Features: {lime_results.get('important_features', [])[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69ab70f",
   "metadata": {},
   "source": [
    "## Step 4: Visualize Results\n",
    "\n",
    "Let's create some visualizations to better understand the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d69d82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Attack Success Rates\n",
    "plt.subplot(2, 3, 1)\n",
    "attacks = ['FGSM', 'PGD']\n",
    "success_rates = [fgsm_results['success_rate'], pgd_results['success_rate']]\n",
    "plt.bar(attacks, success_rates, color=['red', 'orange'])\n",
    "plt.title('Attack Success Rates')\n",
    "plt.ylabel('Success Rate')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Plot 2: Accuracy Comparison\n",
    "plt.subplot(2, 3, 2)\n",
    "categories = ['Original', 'FGSM', 'PGD']\n",
    "accuracies = [\n",
    "    test_accuracy,\n",
    "    fgsm_results['adversarial_accuracy'],\n",
    "    pgd_results['adversarial_accuracy']\n",
    "]\n",
    "plt.bar(categories, accuracies, color=['green', 'red', 'orange'])\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Plot 3: Feature Importance (SHAP)\n",
    "plt.subplot(2, 3, 3)\n",
    "if shap_results.get('feature_importance'):\n",
    "    shap_features = [item['feature'] for item in shap_results['feature_importance'][:8]]\n",
    "    shap_importance = [item['importance'] for item in shap_results['feature_importance'][:8]]\n",
    "    plt.barh(shap_features, shap_importance)\n",
    "    plt.title('SHAP Feature Importance')\n",
    "    plt.xlabel('Importance')\n",
    "\n",
    "# Plot 4: Feature Importance (LIME)\n",
    "plt.subplot(2, 3, 4)\n",
    "if lime_results.get('feature_importance'):\n",
    "    lime_features = [item['feature'] for item in lime_results['feature_importance'][:8]]\n",
    "    lime_importance = [abs(item['importance']) for item in lime_results['feature_importance'][:8]]\n",
    "    plt.barh(lime_features, lime_importance, color='orange')\n",
    "    plt.title('LIME Feature Importance')\n",
    "    plt.xlabel('Importance')\n",
    "\n",
    "# Plot 5: Vulnerability Summary\n",
    "plt.subplot(2, 3, 5)\n",
    "overall_vulnerability = (fgsm_results['success_rate'] + pgd_results['success_rate']) / 2\n",
    "colors = ['green' if overall_vulnerability < 0.3 else 'orange' if overall_vulnerability < 0.6 else 'red']\n",
    "plt.pie([overall_vulnerability, 1-overall_vulnerability], \n",
    "        labels=['Vulnerable', 'Robust'], \n",
    "        colors=[colors[0], 'lightgreen'],\n",
    "        autopct='%1.1f%%')\n",
    "plt.title('Overall Vulnerability')\n",
    "\n",
    "# Plot 6: Data Distribution\n",
    "plt.subplot(2, 3, 6)\n",
    "unique, counts = np.unique(data['y_test'], return_counts=True)\n",
    "plt.bar([f'Class {i}' for i in unique], counts)\n",
    "plt.title('Test Data Distribution')\n",
    "plt.ylabel('Number of Samples')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee11067",
   "metadata": {},
   "source": [
    "## Step 5: Generate Security Report\n",
    "\n",
    "Finally, let's create a comprehensive security analysis report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf52f267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive security report\n",
    "def generate_security_report(attack_results, shap_results, lime_results, data_info):\n",
    "    \"\"\"\n",
    "    Generate a comprehensive security analysis report.\n",
    "    \"\"\"\n",
    "    overall_vulnerability = (attack_results['fgsm']['success_rate'] + \n",
    "                           attack_results['pgd']['success_rate']) / 2\n",
    "    \n",
    "    # Determine risk level\n",
    "    if overall_vulnerability > 0.7:\n",
    "        risk_level = \"HIGH\"\n",
    "        risk_color = \"🔴\"\n",
    "    elif overall_vulnerability > 0.4:\n",
    "        risk_level = \"MEDIUM\"\n",
    "        risk_color = \"🟡\"\n",
    "    else:\n",
    "        risk_level = \"LOW\"\n",
    "        risk_color = \"🟢\"\n",
    "    \n",
    "    report = f\"\"\"\n",
    "    ═══════════════════════════════════════════════════════════\n",
    "                      AI SHIELD SECURITY REPORT\n",
    "    ═══════════════════════════════════════════════════════════\n",
    "    \n",
    "    📊 DATASET SUMMARY\n",
    "    ├─ Total Features: {data_info['statistics']['num_features']}\n",
    "    ├─ Test Samples: {len(data_info['X_test'])}\n",
    "    ├─ Classes: {data_info['num_classes']}\n",
    "    └─ Preprocessing: {'Scaled' if data_info['preprocessing']['scaled'] else 'Not Scaled'}\n",
    "    \n",
    "    🛡️ MODEL ROBUSTNESS ANALYSIS\n",
    "    ├─ Original Accuracy: {attack_results['fgsm']['original_accuracy']:.1%}\n",
    "    ├─ FGSM Attack Success: {attack_results['fgsm']['success_rate']:.1%}\n",
    "    ├─ PGD Attack Success: {attack_results['pgd']['success_rate']:.1%}\n",
    "    └─ Overall Vulnerability: {overall_vulnerability:.1%}\n",
    "    \n",
    "    🔍 EXPLAINABILITY INSIGHTS\n",
    "    ├─ SHAP Analysis: {shap_results.get('explainer_type', 'Completed')}\n",
    "    ├─ LIME Explanations: {lime_results.get('num_explanations', 0)} instances\n",
    "    └─ Top Vulnerable Features: {', '.join(shap_results.get('important_features', [])[:3])}\n",
    "    \n",
    "    ⚠️ RISK ASSESSMENT\n",
    "    ├─ Risk Level: {risk_color} {risk_level}\n",
    "    ├─ Vulnerability Score: {overall_vulnerability:.2f}/1.00\n",
    "    └─ Confidence: High (based on multiple attack vectors)\n",
    "    \n",
    "    📋 RECOMMENDATIONS\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add specific recommendations based on risk level\n",
    "    if risk_level == \"HIGH\":\n",
    "        report += \"\"\"\n",
    "    ├─ 🚨 IMMEDIATE ACTION REQUIRED\n",
    "    ├─ Implement adversarial training with FGSM/PGD examples\n",
    "    ├─ Add input validation and preprocessing\n",
    "    ├─ Consider ensemble methods for robustness\n",
    "    └─ Regular security testing before deployment\n",
    "        \"\"\"\n",
    "    elif risk_level == \"MEDIUM\":\n",
    "        report += \"\"\"\n",
    "    ├─ ⚠️ MODERATE RISK - IMPROVEMENT NEEDED\n",
    "    ├─ Implement defensive distillation\n",
    "    ├─ Add feature preprocessing and normalization\n",
    "    ├─ Monitor for adversarial patterns in production\n",
    "    └─ Consider adversarial training for critical features\n",
    "        \"\"\"\n",
    "    else:\n",
    "        report += \"\"\"\n",
    "    ├─ ✅ LOW RISK - MAINTAIN CURRENT SECURITY\n",
    "    ├─ Continue monitoring for new attack vectors\n",
    "    ├─ Regular security assessments\n",
    "    ├─ Stay updated with latest adversarial techniques\n",
    "    └─ Document current security measures\n",
    "        \"\"\"\n",
    "    \n",
    "    report += \"\"\"\n",
    "    \n",
    "    ═══════════════════════════════════════════════════════════\n",
    "                    Report generated by AI Shield v1.0\n",
    "    ═══════════════════════════════════════════════════════════\n",
    "    \"\"\"\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Generate and display the report\n",
    "security_report = generate_security_report(attack_results, shap_results, lime_results, data)\n",
    "print(security_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf3d127",
   "metadata": {},
   "source": [
    "## Step 6: Save Results\n",
    "\n",
    "Let's save our analysis results for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b756f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Compile all results\n",
    "final_results = {\n",
    "    'metadata': {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'analysis_type': 'adversarial_security_demo',\n",
    "        'ai_shield_version': '1.0.0'\n",
    "    },\n",
    "    'dataset_info': {\n",
    "        'num_samples': len(data['X_test']),\n",
    "        'num_features': len(data['feature_names']),\n",
    "        'num_classes': data['num_classes'],\n",
    "        'feature_names': data['feature_names']\n",
    "    },\n",
    "    'model_performance': {\n",
    "        'train_accuracy': float(train_accuracy),\n",
    "        'test_accuracy': float(test_accuracy)\n",
    "    },\n",
    "    'adversarial_attacks': attack_results,\n",
    "    'xai_explanations': {\n",
    "        'shap': shap_results,\n",
    "        'lime': lime_results\n",
    "    },\n",
    "    'vulnerability_summary': {\n",
    "        'overall_vulnerability_score': (fgsm_results['success_rate'] + pgd_results['success_rate']) / 2,\n",
    "        'risk_level': 'HIGH' if (fgsm_results['success_rate'] + pgd_results['success_rate']) / 2 > 0.7 else 'MEDIUM' if (fgsm_results['success_rate'] + pgd_results['success_rate']) / 2 > 0.4 else 'LOW',\n",
    "        'critical_features': shap_results.get('important_features', [])[:5]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save results to file\n",
    "output_file = '../data/demo_analysis_results.json'\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(final_results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\n✅ Analysis complete! Results saved to: {output_file}\")\n",
    "print(f\"\\n📊 Summary:\")\n",
    "print(f\"   - Overall Vulnerability: {final_results['vulnerability_summary']['overall_vulnerability_score']:.1%}\")\n",
    "print(f\"   - Risk Level: {final_results['vulnerability_summary']['risk_level']}\")\n",
    "print(f\"   - Most Critical Features: {', '.join(final_results['vulnerability_summary']['critical_features'][:3])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55e0483",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates the core capabilities of AI Shield:\n",
    "\n",
    "1. **Adversarial Attack Testing**: We tested model robustness using FGSM and PGD attacks\n",
    "2. **Explainable AI Analysis**: We used SHAP and LIME to understand vulnerabilities\n",
    "3. **Security Assessment**: We generated a comprehensive security report\n",
    "4. **Actionable Insights**: We provided specific recommendations based on risk levels\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- **AWS Integration**: Use the main.py script to run analysis with S3-stored models\n",
    "- **SageMaker Deployment**: Deploy this analysis as a SageMaker processing job\n",
    "- **Advanced Attacks**: Implement additional attack vectors (C&W, AutoAttack)\n",
    "- **Defense Mechanisms**: Implement and test adversarial training\n",
    "\n",
    "### For Production Use:\n",
    "\n",
    "1. Configure AWS credentials and S3 bucket\n",
    "2. Upload your models and datasets to S3\n",
    "3. Run: `python main.py --bucket your-bucket --model path/to/model --data path/to/data`\n",
    "4. Review the generated security report and implement recommendations"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
